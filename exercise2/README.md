# Answers to the questions posed:

## 1. (scrapper) Check the logs for something strange. Try to know where those messages come from. No need to solve it, only get the origin. (0.5 points).
The detected anomaly was excessive noise generated by the metadata library. The anomalous message detected in the initial scraper runs was: "in <ws2:artist>, uncaught attribute type-id" (and similar messages).
These messages originated from the musicbrainzngs library, used to supplement artist information. They occurred because the code logic was designed so that every time an Artist object was created (which happens both when scraping for the first time and when loading the catalog from a JSON file), the fetch_metadata() method was automatically called via the class's __post_init__ event. This resulted in hundreds or thousands of unnecessary calls to the MusicBrainz external API when loading the catalog, generating log noise and slowing down the process.



## 2. Propose any change to make the code cleaner, clearer and better. (0.5 points).
The initial project suffered from coupling issues, network redundancy, and fragility in file handling due to the use of outdated techniques. The following key changes are proposed and implemented to improve code quality, making it more robust, efficient, and maintainable:
- Migration to pathlib for path handling: The tab_cleaner and tab_validator modules manipulated file paths using string operations (.replace(), .split(), os.path.join). This technique was prone to slash errors and the creation of unnecessary directories, as seen in point 4. To address this, path handling was replaced with the standard pathlib module. Paths are now defined as Path objects, and join operations are performed using the / operator, ensuring the correct creation of directories.
- Decoupling Network Logic (Fetch Metadata): The Artist class in utils/data.py called self.fetch_metadata() (accessing the MusicBrainz API) within __post_init__. This violated the single responsibility principle and caused the catalog loading process to make thousands of unnecessary calls to an external API, overloading it and generating log noise (problem 2). To fix this, the call to fetch_metadata() was removed from the constructor and its execution was explicitly moved to the get_artists function in the scraper, which is the only place where that data is needed. This makes the catalog loading process instantaneous and noise-free.
- Eliminating Redundant Scraping: The get_songs function duplicated the cataloging logic, scraping the web again to obtain information that was already stored in catalog.json. To solve this, get_songs was refactored to depend exclusively on the catalog (catalog.json), separating the responsibilities of cataloging (creating metadata) and downloading (using metadata).
- Removal of Mutable Global Variables: The initial code used mutable global variables (such as dir_list and other path strings), which made debugging and scaling difficult. These were removed and replaced with local variables and function return values ​​(for example, list_files_recursive now returns the list of files instead of populating a global variable).
- Centralizing Logging Configuration: The logger configuration was repeated in scraper/main.py, tab_cleaner/main.py, and tab_validator/main.py. This was redundant and dangerous if the log format or level needed to be changed. A utils/log_config.py module containing the configuration function should be created, and each main module would simply call it. This ensures consistency and simplifies maintenance.
- Use dataclasses for configuration: Configurations such as INPUT_DIRECTORY, OUTPUT_DIRECTORY_OK, ROOT, etc., are global constants defined at the module level in each main.py file. These constants can be grouped into a single dataclass within a config.py module. This improves readability and allows for easier configuration management in orchestrator scripts.